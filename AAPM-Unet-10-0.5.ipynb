{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1\n",
    "valid_size = 200\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = np.load(path)\n",
    "    return sample\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.165, 0.275)\n",
    "]) \n",
    "\n",
    "\n",
    "# Datasets\n",
    "train_dataset_Y = datasets.DatasetFolder(root='./AAPM/train/full_dose',\n",
    "                                            loader=npy_loader,\n",
    "                                            extensions=['.npy'],\n",
    "                                           transform = transform)\n",
    "train_dataset_X = datasets.DatasetFolder(root='./AAPM/train/quarter_dose',\n",
    "                                               loader=npy_loader,\n",
    "                                               extensions=['.npy'],\n",
    "                                              transform = transform)\n",
    "\n",
    "\n",
    "test_dataset_Y = datasets.DatasetFolder(root='./AAPM/test/full_dose',\n",
    "                                            loader=npy_loader,\n",
    "                                            extensions=['.npy'],\n",
    "                                           transform = transform)\n",
    "test_dataset_X = datasets.DatasetFolder(root='./AAPM/test/quarter_dose',\n",
    "                                               loader=npy_loader,\n",
    "                                               extensions=['.npy'],\n",
    "                                              transform = transform)\n",
    "\n",
    "# valid_dataset_Y = torch.utils.data.Subset(valid_test_dataset_Y, range(0, 200))\n",
    "# valid_dataset_X = torch.utils.data.Subset(valid_test_dataset_X, range(0, 200))\n",
    "\n",
    "# test_dataset_Y = torch.utils.data.Subset(valid_test_dataset_Y, range(200, 421))\n",
    "# test_dataset_X = torch.utils.data.Subset(valid_test_dataset_X, range(200, 421))\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_Y = torch.utils.data.DataLoader(dataset=train_dataset_Y, batch_size=bs, shuffle=True, drop_last = True)\n",
    "train_loader_X = torch.utils.data.DataLoader(dataset=train_dataset_X, batch_size=bs, shuffle=True, drop_last = True)\n",
    "\n",
    "# valid_loader_Y = torch.utils.data.DataLoader(dataset=valid_dataset_Y, batch_size=1, shuffle=False)\n",
    "# valid_loader_X = torch.utils.data.DataLoader(dataset=valid_dataset_X, batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader_Y = torch.utils.data.DataLoader(dataset=test_dataset_Y, batch_size=1, shuffle=False)\n",
    "test_loader_X = torch.utils.data.DataLoader(dataset=test_dataset_X, batch_size=1, shuffle=False)\n",
    "\n",
    "train_dataset_length = len(train_dataset_X)\n",
    "# valid_dataset_length = len(valid_dataset_X)\n",
    "test_dataset_length = len(test_dataset_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited from https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.inc = DoubleConv(1, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024,256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                   nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.InstanceNorm2d(128),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                   nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "                                   nn.InstanceNorm2d(256),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                   nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "                                   nn.InstanceNorm2d(512),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "                                   nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "                                  )\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "# img_dim = train_dataset.data.shape[1] * train_dataset.data.shape[2] * train_dataset.data.shape[3]\n",
    "\n",
    "# G_Y:X->Y, D_Y:Y->R\n",
    "# G_X:Y->X, D_X:X->R\n",
    "G_Y = Generator().to(device)\n",
    "G_X = Generator().to(device)\n",
    "D_Y = Discriminator().to(device)\n",
    "D_X = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "lambd_cyc = 10\n",
    "lambd_id = 0.5\n",
    "lr = 2e-4\n",
    "patch_size = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "G_Y_optimizer = optim.Adam(G_Y.parameters(), lr = lr)\n",
    "G_X_optimizer = optim.Adam(G_X.parameters(), lr = lr)\n",
    "D_Y_optimizer = optim.Adam(D_Y.parameters(), lr = lr)\n",
    "D_X_optimizer = optim.Adam(D_X.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_train_adv(D, G, x, y, D_optimizer):\n",
    "    criterion = nn.MSELoss()\n",
    "    #=======================Train the discriminator=======================#\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train discriminator on real\n",
    "    D_output_real = D(y)\n",
    "    D_real_loss = criterion(D_output_real, torch.ones(bs, 1, patch_size, patch_size).to(device))\n",
    "\n",
    "    # train discriminator on fake\n",
    "    D_output_fake = D(G(x))\n",
    "    D_fake_loss = criterion(D_output_fake,  torch.zeros(bs, 1, patch_size, patch_size).to(device))\n",
    "    \n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "        \n",
    "    return  D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train_adv(G, D, x, y, G_optimizer):\n",
    "    criterion = nn.MSELoss()\n",
    "    #=======================Train the generator=======================#\n",
    "    G.zero_grad()\n",
    "\n",
    "    D_output_fake = D(G(x))\n",
    "    G_loss = criterion(D_output_fake, torch.ones(bs, 1, patch_size, patch_size).to(device))\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "        \n",
    "    return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train_cyc(G_X, G_Y, x, y, G_X_optimizer, G_Y_optimizer):\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    G_X.zero_grad()\n",
    "    G_Y.zero_grad()\n",
    "    \n",
    "    cyc_loss = lambd_cyc * (criterion(G_X(G_Y(x)), x) + criterion(G_Y(G_X(y)), y))\n",
    "    \n",
    "    cyc_loss.backward()\n",
    "    G_X_optimizer.step()\n",
    "    G_Y_optimizer.step()\n",
    "    \n",
    "    return cyc_loss.data.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train_id(G_X, G_Y, x, y, G_X_optimizer, G_Y_optimizer):\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    G_X.zero_grad()\n",
    "    G_Y.zero_grad()\n",
    "    \n",
    "    id_loss = lambd_id * (criterion(G_Y(x), x) + criterion(G_X(y), y))\n",
    "    \n",
    "    id_loss.backward()\n",
    "    G_X_optimizer.step()\n",
    "    G_Y_optimizer.step()\n",
    "    \n",
    "    return id_loss.data.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_orig_PSNR(x, y):\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        MSE = criterion(x, y)\n",
    "        orig_PSNR = 10 * torch.log((y.max() - y.min())**2 / MSE) / math.log(10)\n",
    "    return orig_PSNR\n",
    "\n",
    "def calc_orig_PSNR_loader(loader_X, loader_Y):\n",
    "    orig_PSNR_list = []\n",
    "    for x, y in zip(loader_X, loader_Y):\n",
    "        x = x[0].to(device) # remove labels\n",
    "        y = y[0].to(device)\n",
    "        orig_PSNR_list.append(calc_orig_PSNR(x,y))\n",
    "    orig_PSNR_loader = torch.mean(torch.stack(orig_PSNR_list)).item()\n",
    "    return orig_PSNR_loader\n",
    "\n",
    "train_orig_PSNR = calc_orig_PSNR_loader(train_loader_X, train_loader_Y)\n",
    "test_orig_PSNR = calc_orig_PSNR_loader(test_loader_X, test_loader_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_PSNR(G_Y, x, y):\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        MSE = criterion(G_Y(x), y)\n",
    "        PSNR = 10 * torch.log((y.max() - y.min())**2 / MSE) / math.log(10)\n",
    "    return PSNR\n",
    "\n",
    "def calc_PSNR_loader(G_Y, loader_X, loader_Y):\n",
    "    PSNR_list = []\n",
    "    for x, y in zip(loader_X, loader_Y):\n",
    "        x = x[0].to(device) # remove labels\n",
    "        y = y[0].to(device)\n",
    "        PSNR_list.append(calc_PSNR(G_Y, x,y))\n",
    "    PSNR_loader = torch.mean(torch.stack(PSNR_list)).item()\n",
    "    return PSNR_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3839/3839 [1:21:18<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100]: loss_d: 0.999, loss_g: 0.503, loss_cyc: 0.023, loss_id: 0.003, test_PSNR_diff: 3.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████                           | 1184/3839 [25:08<56:03,  1.27s/it]"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "if 'epoch' in locals():\n",
    "    if complete:\n",
    "        init_epoch = epoch+1\n",
    "    else:\n",
    "        init_epoch = epoch\n",
    "else:\n",
    "    init_epoch = 1\n",
    "if not 'max_PSNR_diff' in locals():\n",
    "    max_PSNR_diff = float('-inf')\n",
    "\n",
    "writer = SummaryWriter(log_dir = 'runs/' + str(lambd_cyc) + '_' + str(lambd_id))\n",
    "\n",
    "D_X_losses,G_X_losses,D_Y_losses,G_Y_losses,cyc_losses, id_losses, train_PSNRs, valid_PSNRs = deque(),deque(),deque(),deque(),deque(),deque(),deque(),deque()\n",
    "\n",
    "for epoch in range(init_epoch, init_epoch+n_epoch):\n",
    "    complete = False # marker for whether the training completed the last epoch\n",
    "    \n",
    "    for step, (x, y) in tqdm(enumerate(zip(train_loader_X, train_loader_Y)),total=train_dataset_length):\n",
    "        x = x[0].to(device) # remove labels\n",
    "        y = y[0].to(device)\n",
    "        \n",
    "        D_X_losses.append(D_train_adv(D_X, G_X, x, y, D_X_optimizer))\n",
    "        G_X_losses.append(G_train_adv(G_X, D_X, x, y, G_X_optimizer))\n",
    "        D_Y_losses.append(D_train_adv(D_Y, G_Y, x, y, D_Y_optimizer))\n",
    "        G_Y_losses.append(G_train_adv(G_Y, D_Y, x, y, G_Y_optimizer))\n",
    "        cyc_losses.append(G_train_cyc(G_X, G_Y, x, y, G_X_optimizer, G_Y_optimizer))\n",
    "        id_losses.append(G_train_id(G_X, G_Y, x, y, G_X_optimizer, G_Y_optimizer))\n",
    "        train_PSNRs.append(calc_PSNR(G_Y, x, y))\n",
    "        \n",
    "        if len(D_X_losses) > train_dataset_length:\n",
    "            D_X_losses.popleft()\n",
    "            G_X_losses.popleft()\n",
    "            D_Y_losses.popleft()\n",
    "            G_Y_losses.popleft()\n",
    "            cyc_losses.popleft()\n",
    "            id_losses.popleft()\n",
    "            train_PSNRs.popleft()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            loss_D = torch.mean(torch.FloatTensor(D_X_losses) + torch.FloatTensor(D_Y_losses)).item()\n",
    "            loss_G = torch.mean(torch.FloatTensor(G_X_losses)+torch.FloatTensor(G_Y_losses)).item()\n",
    "            loss_cyc = torch.mean(torch.FloatTensor(cyc_losses)).item()\n",
    "            loss_id = torch.mean(torch.FloatTensor(id_losses)).item()\n",
    "            train_PSNR = torch.mean(torch.FloatTensor(train_PSNRs)).item()\n",
    "            train_PSNR_diff = train_PSNR - train_orig_PSNR\n",
    "            \n",
    "            step_total = (epoch-1)*train_dataset_length + step\n",
    "            writer.add_scalar('loss_D', loss_D, step_total)\n",
    "            writer.add_scalar('loss_G', loss_G, step_total)\n",
    "            writer.add_scalar('loss_cyc', loss_cyc, step_total)\n",
    "            writer.add_scalar('loss_id', loss_id, step_total)\n",
    "            writer.add_scalar('train_PSNR_diff', train_PSNR_diff, step_total)\n",
    "    \n",
    "    test_PSNR = calc_PSNR_loader(G_Y, test_loader_X, test_loader_Y)\n",
    "    test_PSNR_diff = test_PSNR - test_orig_PSNR\n",
    "    \n",
    "    # valid_PSNR = calc_PSNR_loader(G_Y, valid_loader_X, valid_loader_Y)\n",
    "    # valid_PSNR_diff = valid_PSNR - valid_orig_PSNR\n",
    "    \n",
    "    writer.add_scalar('test_PSNR_diff', test_PSNR_diff, epoch)\n",
    "    \n",
    "    if test_PSNR_diff > max_PSNR_diff:\n",
    "        max_PSNR_diff = test_PSNR_diff\n",
    "        torch.save({\n",
    "            'G_X': G_X.state_dict(),\n",
    "            'G_Y': G_Y.state_dict(),\n",
    "            'D_X': D_X.state_dict(),\n",
    "            'D_Y': D_Y.state_dict(),\n",
    "            'G_X_optimizer': G_X_optimizer.state_dict(),\n",
    "            'G_Y_optimizer': G_Y_optimizer.state_dict(),\n",
    "            'D_X_optimizer': D_X_optimizer.state_dict(),\n",
    "            'D_Y_optimizer': D_Y_optimizer.state_dict(),\n",
    "        }, \"unet_best_\" + str(lambd_cyc) + '_' + str(lambd_id) + \".pt\")\n",
    "        \n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f, loss_cyc: %.3f, loss_id: %.3f, test_PSNR_diff: %.3f' % (\n",
    "            epoch, n_epoch, loss_D, loss_G, loss_cyc, loss_id, test_PSNR_diff))\n",
    "    complete = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"unet_best_10_5.pt\")\n",
    "G_X.load_state_dict(checkpoint['G_X'])\n",
    "G_Y.load_state_dict(checkpoint['G_Y'])\n",
    "D_X.load_state_dict(checkpoint['D_X'])\n",
    "D_Y.load_state_dict(checkpoint['D_Y'])\n",
    "G_X_optimizer.load_state_dict(checkpoint['G_X_optimizer'])\n",
    "G_Y_optimizer.load_state_dict(checkpoint['G_Y_optimizer'])\n",
    "D_X_optimizer.load_state_dict(checkpoint['D_X_optimizer'])\n",
    "D_Y_optimizer.load_state_dict(checkpoint['D_Y_optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PSNR diffrence on test set is 4.366001129150391\n"
     ]
    }
   ],
   "source": [
    "test_PSNR = calc_PSNR_loader(G_Y, test_loader_X, test_loader_Y)\n",
    "print(\"The PSNR diffrence on test set is \" + str(test_PSNR - test_orig_PSNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR_list_loader(G_Y, loader_X, loader_Y):\n",
    "    PSNR_list = []\n",
    "    for x, y in zip(loader_X, loader_Y):\n",
    "        x = x[0].to(device) # remove labels\n",
    "        y = y[0].to(device)\n",
    "        PSNR_list.append(calc_PSNR(G_Y, x,y))\n",
    "    return PSNR_list\n",
    "\n",
    "def orig_PSNR_list_loader(loader_X, loader_Y):\n",
    "    orig_PSNR_list = []\n",
    "    for x, y in zip(loader_X, loader_Y):\n",
    "        x = x[0].to(device) # remove labels\n",
    "        y = y[0].to(device)\n",
    "        orig_PSNR_list.append(calc_orig_PSNR(x,y))\n",
    "    return orig_PSNR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PSNR_list = PSNR_list_loader(G_Y, test_loader_X, test_loader_Y)\n",
    "test_orig_PSNR_list = orig_PSNR_list_loader(test_loader_X, test_loader_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((torch.stack(test_PSNR_list) - torch.stack(test_orig_PSNR_list)).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_list = torch.stack(test_PSNR_list) - torch.stack(test_orig_PSNR_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = torch.argmax(diff_list)\n",
    "worst_index = torch.argmin(diff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [best_index, worst_index]:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    x = test_dataset_X[i][0].view(1,1,512,512)\n",
    "    y_pred = G_Y(x.to(device)).cpu()\n",
    "    x = x.view(1,512,512)\n",
    "    y_pred = y_pred.view(1,512,512)\n",
    "    y = test_dataset_Y[i][0].view(1,512,512)\n",
    "\n",
    "    diff = y_pred - x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        MSE = criterion(y_pred, y)\n",
    "        PSNR = 10 * torch.log((y.max() - y.min())**2 / MSE) / math.log(10)\n",
    "        MSE_orig = criterion(x, y)\n",
    "        PSNR_orig = 10 * torch.log((y.max() - y.min())**2 / MSE_orig) / math.log(10)\n",
    "\n",
    "\n",
    "    save_folder = str(round(PSNR.item(),1)) + '_' + str(round(PSNR_orig.item(),1))\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    save_image(x, save_folder + '/x.png', normalize=True)\n",
    "    save_image(y_pred, save_folder + '/y_pred.png', normalize=True)\n",
    "    save_image(y, save_folder + '/y.png', normalize=True)\n",
    "    save_image(diff, save_folder + '/diff.png', normalize=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
